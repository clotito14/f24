\documentclass[11pt]{article}

\usepackage{graphicx}			% Use this package to include images
\usepackage{amsmath}			% A library of many standard math expressions
\usepackage{amsfonts}
\usepackage[margin=1in]{geometry}% Sets 1in margins.
\usepackage{fancyhdr}			% Creates headers and footers
\usepackage{enumerate}          %These two package give custom labels to a list
\usepackage[shortlabels]{enumitem}
\usepackage{braket}
\usepackage{physics}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{url}
\usepackage{listings}
\pgfplotsset{compat=1.18}

%% LISTINGS CONFIG %%

\definecolor{purple2}{RGB}{153,0,153} % there's actually no standard purple
\definecolor{green2}{RGB}{0,153,0} % a darker green

\lstset{
  language=Python,                   % the language
  basicstyle=\normalsize\ttfamily,   % size of the fonts for the code
  frame = single,
  % Color settings to match IDLE style
  keywordstyle=\color{orange},       % core keywords
  keywordstyle={[2]\color{purple2}}, % built-ins
  stringstyle=\color{green2},%
  showstringspaces=false,
  commentstyle=\color{red},%
  upquote=true,                      % requires textcomp
  numbers=left,
  breaklines=true,
}

% Creates the header and footer. You can adjust the look and feel of these here.
\pagestyle{fancy}
\fancyhead[l]{Chase A. Lotito}
\fancyhead[c]{ECE469 Homework \#1}
\fancyhead[r]{\today}
\fancyfoot[c]{\thepage}
\renewcommand{\headrulewidth}{0.2pt} %Creates a horizontal line underneath the header
\setlength{\headheight}{15pt} %Sets enough space for the header



\begin{document} %The writing for your homework should all come after this.

%Enumerate starts a list of problems so you can put each homework problem after each item.
\begin{enumerate}[start=1,label={\bfseries Question \arabic*:},leftmargin=1in] %You can change "Problem" to be whatevber label you like.

    \item Select ALL correct choices.
    
    \textbf{[1.1]}

    \textbf{[1.2]}
    
    \textbf{[1.3]}
    
    \textbf{[1.4]}
    
    \textbf{[1.5]}

    \item A linear ML model can be written as: \[f(\vb{x}, \vb{w}) = \sum_{i=0}^n w_i x_i = \vb{w}^T \vb{x}\] The loss function can be written as: \[ J(\vb{w}) = \frac{1}{m} \sum_{i=1}^n \left[ f(\vb{x}^{(i)}, \vb{w}) - y^{(i)} \right]^2 \]

        \textbf{[2.1]} Show analytically that the optimal weight vector that minimizes the cost function \(J(\vb{w})\) is: \[\vb{w}^* = \left( \vb{X}^T\vb{X}  \right)^{-1} \vb{X}^T\vb{y} \]

        \noindent \textbf{Solution.}

        \noindent Since our model is linear, we can write the cost function as:

        \begin{equation}
            J(\vb{w}) = \frac{1}{m} \sum_{i=1}^n \left[ \vb{w}^T \vb{x}^{(i)} - y^{(i)} \right]^2
            \label{eq:linear-cost-vectors}
        \end{equation}

        The product \(\vb{w}^T \vb{x}^{(i)} = \vb{Xw}, \forall i \in \set{1, 2, \cdots, n}\) since the LHS implies the matrix mulplitcation of the RHS, as \(\vb{X}\) is the matrix of all input entries \(\vb{x}^{(i)}\). So, Eq. \ref{eq:linear-cost-vectors} can be written as:

        \begin{equation}
            J(\vb{w}) = \frac{1}{m} \left[ \vb{Xw}  - \vb{y} \right]^2
            \label{eq:linear-cost-sum-matrix}
        \end{equation}

        The inside of the brackets is just a vector, and the square of a vector is the norm of a vector, so we can reduce Eq. \ref{eq:linear-cost-sum-matrix}:

        \begin{align}
            J(\vb{w}) &= \frac{1}{m} \norm{ \vb{Xw} - \vb{y}  } \\
                      &= \frac{1}{m} (\vb{Xw} - \vb{y})^T(\vb{Xw} - \vb{y}) \\
                      &= \frac{1}{m} ( \vb{w}^T\vb{X}^T\vb{Xw} - \vb{w}^T\vb{X}^T\vb{y} - \vb{y}^T \vb{Xw} + \vb{y}^T\vb{y} )
        \end{align}

        Now to optimize the cost with respect to weights, we can take the gradient of \(J(\vb{w})\) w.r.t. the weights \(\vb{w}^{(i)}\).

        \begin{align}
            \nabla_{\vb{w}} J(\vb{w}) &= \frac{1}{m} \nabla_{\vb{w}} (  \vb{w}^T\vb{X}^T\vb{Xw} - \vb{w}^T\vb{X}^T\vb{y} - \vb{y}^T \vb{Xw} + \vb{y}^T\vb{y} ) \\
                                      &= \frac{1}{m} \nabla_{\vb{w}} (  \vb{w}^T\vb{X}^T\vb{Xw} - \vb{w}^T\vb{X}^T\vb{y} - \vb{y}^T \vb{Xw}  ) \\
                                      &= \frac{1}{m} [ \nabla_{\vb{w}} (  \vb{w}^T\vb{X}^T\vb{Xw } ) - \nabla_{\vb{w}} ( \vb{w}^T\vb{X}^T\vb{y} ) - \nabla_{\vb{w}} ( \vb{y}^T \vb{Xw}  ) ) ]
        \end{align}

        The gradients \(\nabla_{\vb{w}}\) are simply derivatives of each matrix function w.r.t. \(\vb{w}\), which can be computed using equations (69) and (81) from \emph{The Matrix Cookbook} \cite{matrixcookbook}.

        \begin{align}
            &= \frac{1}{m} \left[ \pdv{\vb{w}} (  \vb{w}^T\vb{X}^T\vb{Xw } ) - \pdv{\vb{w}} ( \vb{w}^T\vb{X}^T\vb{y} ) - \pdv{\vb{w}} ( \vb{y}^T \vb{Xw}  ) ) \right] \\
            &= \frac{1}{m} ( (\vb{X}^T\vb{X} + ( \vb{X}^T\vb{X} )^T )\vb{w} - \vb{X}^T\vb{y} - \vb{y}^T\vb{X} ) \\
            &= \frac{1}{m} ( (\vb{X}^T\vb{X} + \vb{X}^T ( \vb{X}^T )^T )\vb{w} - 2\vb{X}^T\vb{y} ) \\
            &= \frac{1}{m} ( (\vb{X}^T\vb{X} + \vb{X}^T\vb{X})\vb{w} - 2\vb{X}^T\vb{y} ) \\
            &= \frac{1}{m} (2\vb{X}^T\vb{X}\vb{w} - 2\vb{X}^T\vb{y} ) \\
            &= \frac{2}{m} (\vb{X}^T\vb{X}\vb{w} - \vb{X}^T\vb{y} )
        \end{align}

        We find the minimum when \(\nabla_{\vb{w}} J (\vb{w}) = 0\),

        \begin{align}
            \frac{2}{m} (\vb{X}^T\vb{X}\vb{w} - \vb{X}^T\vb{y} ) &= 0 \\
            \vb{X}^T\vb{X}\vb{w} - \vb{X}^T\vb{y} &= 0 \\
            \vb{X}^T\vb{X}\vb{w} &= \vb{X}^T\vb{y} \\
            (\vb{X}^T\vb{X})^{-1} ( \vb{X}^T\vb{X}\vb{w} ) &= (\vb{X}^T\vb{X})^{-1} ( \vb{X}^T\vb{y}) \\
            \vb{I}\vb{w} &= (\vb{X}^T\vb{X})^{-1} \vb{X}^T\vb{y} \\
            \Aboxed{\vb{w} &= (\vb{X}^T\vb{X})^{-1} \vb{X}^T\vb{y} = \vb{w}^*}
        \end{align}

        Without advanced methods, multiplying two \((n\times n)\) matrices is of \(O(n^3)\) complexity \cite{matrixcompute}. For very large data sets, computing \(\vb{w}\) would be enormously computationally expensive.
        
        \textbf{[2.2]} Develop a pseudo-codes for implementing the batch gradient descent, stochastic gradient descent, and mini-batch gradient descent algorithms to train the above linear model.

        \textbf{Solution.}

        \begin{lstlisting}
            
        \end{lstlisting}
        
        % Question 3
        \item Housing data preprocessing.
        
        \textbf{Solution.}

        \begin{lstlisting}
# Chase Lotito - SIUC Fall 2024 - ECE469: Intro to Machine Learning
# HW1 - Question 3

import numpy as np
import pandas as pd
from sklearn.preprocessing import OrdinalEncoder    # For encoding categorical features
from sklearn.impute import SimpleImputer            # For adding missing values
from sklearn.preprocessing import StandardScaler    # For standardizing data

# (A) Get housing data
RAW_DATA = 'https://github.com/ageron/data/raw/main/housing/housing.csv'
housing = pd.read_csv(RAW_DATA)

# (B) Choose input features and output features (saved into numpy.ndarray type)
X = housing[
        ['longitude',
        'latitude',
        'housing_median_age',
        'total_rooms',
        'total_bedrooms',
        'population',
        'households',
        'median_income',
        'ocean_proximity']
    ].values
Y = housing[['median_house_value']].values

# (C) Ocean Proximity is a categorical feature. Drop it or transform into numerical values (encode).

# Isolate the ocean_proximity data in input data X
ocean_proximity = X[:,8].reshape(-1,1)   # reshape(-1,1) to make 2D array for Ordinal
# Initalize the ordinal encoder
ordinal_encoder = OrdinalEncoder()
# Encode the ocean_proximity strings into numerical data
encoded_ocean = ordinal_encoder.fit_transform(ocean_proximity)
# Put the encoded version of ocean_proximity into input data X
X[:,8] = encoded_ocean.flatten()         # flatten to add 1D version of array back into X

# (D) Clean the dasta by either dropping or replacing missing values

# Initialized SimpleImputer, will use the median to add missing entries
simple_imputer = SimpleImputer(strategy='median')

# Change X np ndarray into a Pandas Dataframe to use SimpleImputer
dX = pd.DataFrame(X)
dY = pd.DataFrame(Y)

# Perform SimpleImputer transformation, for both inputs X and outputs Y
imputed_data = simple_imputer.fit_transform(dX)
X = imputed_data
imputed_data = simple_imputer.fit_transform(dY)
Y = imputed_data

# (E) Carry out feature scaling either via normalization or standardization.
std_scaler = StandardScaler()
scaled_data = std_scaler.fit_transform(X)
X = scaled_data
scaled_data = std_scaler.fit_transform(Y)
Y = scaled_data

# (F) Create a training dataset and testing dataset
def shuffle_and_split_data(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]

# first, recombine X and Y into augmented matrix
housing_data = np.hstack((X,Y))

# split into testing and training set (both outputted as pd.DataFrames)
housing_training, housing_testing = shuffle_and_split_data(pd.DataFrame(housing_data), 0.2)
print('TRAINING:')
print(housing_training)
print('TESTING:')
print(housing_testing)
        \end{lstlisting}

\end{enumerate}

% REFERENCES!
\bibliographystyle{Plain}
\bibliography{bib.bib}

\end{document}
